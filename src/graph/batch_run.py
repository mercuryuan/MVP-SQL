import os
import time
import logging
from pathlib import Path
from tqdm import tqdm
import sys

# Add project root to Python path
project_root = Path(__file__).resolve().parent.parent.parent
if str(project_root) not in sys.path:
    sys.path.insert(0, str(project_root))

# å¼•å…¥ä½ çš„æ–°æ¨¡å—
from pipeline import SchemaPipeline
from configs import paths

# é…ç½®æ—¥å¿—
logging.basicConfig(
    filename="pipeline_batch_run.log",
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(message)s",
    encoding='utf-8'
)


def process_dataset(dataset_name, dataset_root_path, skip_existing=False):
    """
    æ‰¹é‡å¤„ç†æŒ‡å®šæ•°æ®é›†ä¸‹çš„æ‰€æœ‰æ•°æ®åº“ã€‚

    :param dataset_name: æ•°æ®é›†åç§° (e.g., 'bird', 'spider')ï¼Œç”¨äºç”Ÿæˆè¾“å‡ºç›®å½•å±‚çº§
    :param dataset_root_path: æ•°æ®é›†æ ¹ç›®å½• (åŒ…å«å„ä¸ªæ•°æ®åº“æ–‡ä»¶å¤¹çš„ç›®å½•)
    :param skip_existing: å¦‚æœç›®æ ‡ pkl æ–‡ä»¶å·²å­˜åœ¨ï¼Œæ˜¯å¦è·³è¿‡
    """
    root_dir = Path(dataset_root_path)

    if not root_dir.exists():
        print(f"âŒ é”™è¯¯: æ•°æ®é›†è·¯å¾„ä¸å­˜åœ¨: {root_dir}")
        return

    # 1. æ‰«ææ‰€æœ‰å­ç›®å½•ï¼Œå¯»æ‰¾ .sqlite æ–‡ä»¶
    # å‡è®¾ç»“æ„: root / db_name / db_name.sqlite
    db_dirs = [d for d in root_dir.iterdir() if d.is_dir()]

    print(f"\nğŸš€ å¼€å§‹å¤„ç†æ•°æ®é›†: [{dataset_name}]")
    print(f"ğŸ“‚ æ‰«æåˆ° {len(db_dirs)} ä¸ªæ•°æ®åº“æ–‡ä»¶å¤¹")
    print(f"ğŸ“‚ è¾“å‡ºæ ¹ç›®å½•: {paths.OUTPUT_ROOT}")

    # æ‰“å°è·³è¿‡æ¨¡å¼çŠ¶æ€
    if skip_existing:
        print("â© å·²å¼€å¯æ–­ç‚¹ç»­ä¼ æ¨¡å¼ï¼šæ£€æµ‹åˆ°ç›®æ ‡æ–‡ä»¶å­˜åœ¨å°†è‡ªåŠ¨è·³è¿‡")

    success_count = 0
    fail_count = 0
    skip_count = 0  # æ–°å¢ç»Ÿè®¡

    # ä½¿ç”¨ tqdm æ˜¾ç¤ºè¿›åº¦
    pbar = tqdm(db_dirs, desc=f"Building Graphs ({dataset_name})", unit="db")

    for db_dir in pbar:
        db_name = db_dir.name

        # å¯»æ‰¾è¯¥ç›®å½•ä¸‹çš„ sqlite æ–‡ä»¶
        sqlite_files = list(db_dir.glob("*.sqlite"))

        if not sqlite_files:
            logging.warning(f"Skipping {db_name}: No .sqlite file found.")
            continue

        # é»˜è®¤å–ç¬¬ä¸€ä¸ª sqlite æ–‡ä»¶
        sqlite_path = sqlite_files[0]

        # 2. æ„å»ºè¾“å‡ºè·¯å¾„
        # ç»“æ„: output / dataset / db_name / db_name.pkl
        output_dir = paths.OUTPUT_ROOT / dataset_name / db_name
        output_pkl = output_dir / f"{db_name}.pkl"

        # === æ ¸å¿ƒä¿®æ”¹ï¼šæ£€æµ‹å­˜åœ¨åˆ™è·³è¿‡ ===
        if skip_existing and output_pkl.exists():
            skip_count += 1
            # è®°å½•æ—¥å¿—ï¼Œå¹¶åœ¨è¿›åº¦æ¡åç¼€æ˜¾ç¤ºçŠ¶æ€ï¼Œä½†ä¸æ‰“å°åˆ·å±
            logging.info(f"Skipping {db_name}: Output file already exists -> {output_pkl}")
            pbar.set_postfix(status="Skipped", db=db_name)
            continue
        # ============================

        # 3. æ‰§è¡Œ Pipeline
        try:
            # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
            output_dir.mkdir(parents=True, exist_ok=True)

            # æ›´æ–°è¿›åº¦æ¡æè¿°
            pbar.set_postfix(status="Processing", db=db_name)

            # è¿™é‡Œçš„ SchemaPipeline å°è£…äº†æ‰€æœ‰ç»†èŠ‚ï¼šSQLiteè¯»å– -> åˆ†æ -> æ„å»ºå›¾ -> ä¿å­˜
            pipeline = SchemaPipeline(str(sqlite_path), str(output_pkl))
            pipeline.run()  # å†…éƒ¨å·²ç»åŒ…å«äº† tqdm (åˆ—çº§åˆ«)

            success_count += 1
            logging.info(f"Success: {db_name} -> {output_pkl}")

        except Exception as e:
            fail_count += 1
            error_msg = f"Failed: {db_name}. Error: {str(e)}"
            logging.error(error_msg)

    print(f"\nâœ… [{dataset_name}] å¤„ç†å®Œæˆ Summary:")
    print(f"   - æˆåŠŸ: {success_count}")
    print(f"   - å¤±è´¥: {fail_count}")
    print(f"   - è·³è¿‡: {skip_count}")  # è¾“å‡ºè·³è¿‡æ•°é‡
    print(f"   - æ—¥å¿—å·²ä¿å­˜è‡³ pipeline_batch_run.log")


if __name__ == "__main__":
    # ================= é…ç½®åŒºåŸŸ =================

    # 1. BIRD æ•°æ®é›†é…ç½®
    # æ³¨æ„ï¼šè¿™é‡Œä½¿ç”¨ getattr æ˜¯ä¸ºäº†é˜²æ­¢ paths.py ä¸­æ²¡æœ‰å®šä¹‰ TRAIN_BIRD å¯¼è‡´æŠ¥é”™ï¼Œé»˜è®¤å›é€€åˆ°ä½ ä¹‹å‰çš„è·¯å¾„
    bird_path = getattr(paths, "TRAIN_BIRD", r"F:\train_bird\train_databases")

    # 2. SPIDER æ•°æ®é›†é…ç½®
    # spider_path = getattr(paths, "SPIDER_TRAIN", r"../data/spider/database")

    # ================= æ‰§è¡ŒåŒºåŸŸ =================

    # æ‰§è¡Œ BIRD
    if os.path.exists(bird_path):
        process_dataset(
            dataset_name="bird",
            dataset_root_path=bird_path,
            skip_existing=True  # ã€ä¿®æ”¹ã€‘å¼€å¯è·³è¿‡æ¨¡å¼ï¼Œé¿å…è¦†ç›–ç”Ÿæˆ
        )
    else:
        print(f"âŒ æœªæ‰¾åˆ° BIRD æ•°æ®é›†è·¯å¾„: {bird_path}")

    # æ‰§è¡Œ SPIDER (ç¨åé…ç½®å¥½è·¯å¾„åå–æ¶ˆæ³¨é‡Š)
    # if os.path.exists(spider_path):
    #     process_dataset("spider", spider_path, skip_existing=True)
