import os
import time
import logging
from pathlib import Path
from tqdm import tqdm

# å¼•å…¥ä½ çš„æ–°æ¨¡å—
from pipeline import SchemaPipeline
from configs import paths

# é…ç½®æ—¥å¿—
logging.basicConfig(
    filename="pipeline_batch_run.log",
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(message)s",
    encoding='utf-8'
)


def process_dataset(dataset_name, dataset_root_path, skip_existing=False):
    """
    æ‰¹é‡å¤„ç†æŒ‡å®šæ•°æ®é›†ä¸‹çš„æ‰€æœ‰æ•°æ®åº“ã€‚

    :param dataset_name: æ•°æ®é›†åç§° (e.g., 'bird', 'spider')ï¼Œç”¨äºç”Ÿæˆè¾“å‡ºç›®å½•å±‚çº§
    :param dataset_root_path: æ•°æ®é›†æ ¹ç›®å½• (åŒ…å«å„ä¸ªæ•°æ®åº“æ–‡ä»¶å¤¹çš„ç›®å½•)
    :param skip_existing: å¦‚æœç›®æ ‡ pkl æ–‡ä»¶å·²å­˜åœ¨ï¼Œæ˜¯å¦è·³è¿‡
    """
    root_dir = Path(dataset_root_path)

    if not root_dir.exists():
        print(f"âŒ é”™è¯¯: æ•°æ®é›†è·¯å¾„ä¸å­˜åœ¨: {root_dir}")
        return

    # 1. æ‰«ææ‰€æœ‰å­ç›®å½•ï¼Œå¯»æ‰¾ .sqlite æ–‡ä»¶
    # å‡è®¾ç»“æ„: root / db_name / db_name.sqlite
    db_dirs = [d for d in root_dir.iterdir() if d.is_dir()]

    print(f"\nğŸš€ å¼€å§‹å¤„ç†æ•°æ®é›†: [{dataset_name}]")
    print(f"ğŸ“‚ æ‰«æåˆ° {len(db_dirs)} ä¸ªæ•°æ®åº“æ–‡ä»¶å¤¹")
    print(f"ğŸ“‚ è¾“å‡ºæ ¹ç›®å½•: {paths.OUTPUT_ROOT}")

    success_count = 0
    fail_count = 0

    # ä½¿ç”¨ tqdm æ˜¾ç¤ºè¿›åº¦
    pbar = tqdm(db_dirs, desc=f"Building Graphs ({dataset_name})", unit="db")

    for db_dir in pbar:
        db_name = db_dir.name

        # å¯»æ‰¾è¯¥ç›®å½•ä¸‹çš„ sqlite æ–‡ä»¶ (é€šå¸¸æ–‡ä»¶åä¸æ–‡ä»¶å¤¹åä¸€è‡´ï¼Œä½†ä¹Ÿå¯èƒ½ä¸ä¸€è‡´ï¼Œè¿™é‡Œåšä¸ªæ¨¡ç³ŠåŒ¹é…)
        sqlite_files = list(db_dir.glob("*.sqlite"))

        if not sqlite_files:
            logging.warning(f"Skipping {db_name}: No .sqlite file found.")
            continue

        # é»˜è®¤å–ç¬¬ä¸€ä¸ª sqlite æ–‡ä»¶
        sqlite_path = sqlite_files[0]

        # 2. æ„å»ºè¾“å‡ºè·¯å¾„
        # ç»“æ„: output / dataset / db_name / db_name.pkl
        output_dir = paths.OUTPUT_ROOT / dataset_name / db_name
        output_pkl = output_dir / f"{db_name}.pkl"

        # å¢é‡å¤„ç†é€»è¾‘
        if skip_existing and output_pkl.exists():
            continue

        # 3. æ‰§è¡Œ Pipeline
        try:
            # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
            output_dir.mkdir(parents=True, exist_ok=True)

            # æ›´æ–°è¿›åº¦æ¡æè¿°
            pbar.set_postfix(db=db_name)

            # === æ ¸å¿ƒè°ƒç”¨ ===
            # è¿™é‡Œçš„ SchemaPipeline å°è£…äº†æ‰€æœ‰ç»†èŠ‚ï¼šSQLiteè¯»å– -> åˆ†æ -> æ„å»ºå›¾ -> ä¿å­˜
            pipeline = SchemaPipeline(str(sqlite_path), str(output_pkl))
            pipeline.run()  # å†…éƒ¨å·²ç»åŒ…å«äº† tqdm (åˆ—çº§åˆ«)ï¼Œå¯èƒ½ä¼šæœ‰åŒé‡è¿›åº¦æ¡ï¼Œè§†æƒ…å†µè°ƒæ•´
            # ===============

            success_count += 1
            logging.info(f"Success: {db_name} -> {output_pkl}")

        except Exception as e:
            fail_count += 1
            error_msg = f"Failed: {db_name}. Error: {str(e)}"
            logging.error(error_msg)
            # åœ¨æ§åˆ¶å°æ‰“å°ç®€çŸ­é”™è¯¯ï¼Œè¯¦ç»†é”™è¯¯è¿›æ—¥å¿—
            # tqdm.write(f"âŒ {db_name} å¤±è´¥: {e}")

    print(f"\nâœ… [{dataset_name}] å¤„ç†å®Œæˆ Summary:")
    print(f"   - æˆåŠŸ: {success_count}")
    print(f"   - å¤±è´¥: {fail_count}")
    print(f"   - æ—¥å¿—å·²ä¿å­˜è‡³ pipeline_batch_run.log")


if __name__ == "__main__":
    # ================= é…ç½®åŒºåŸŸ =================

    # 1. BIRD æ•°æ®é›†é…ç½®
    # è¯·ä¿®æ”¹ä¸ºä½ å®é™…çš„ BIRD æ•°æ®é›†è·¯å¾„

    # 2. SPIDER æ•°æ®é›†é…ç½®
    # è¯·ä¿®æ”¹ä¸ºä½ å®é™…çš„ SPIDER æ•°æ®é›†è·¯å¾„
    # SPIDER_TRAIN = r"../data/spider/database"

    # ================= æ‰§è¡ŒåŒºåŸŸ =================

    # æ‰§è¡Œ BIRD
    if os.path.exists(paths.TRAIN_BIRD):
        process_dataset(
            dataset_name="bird",
            dataset_root_path=paths.TRAIN_BIRD,
            skip_existing=False  # è®¾ä¸º True å¯ä»¥æ–­ç‚¹ç»­ä¼ 
        )

    # æ‰§è¡Œ SPIDER (ç¨åé…ç½®å¥½è·¯å¾„åå–æ¶ˆæ³¨é‡Š)
    # if os.path.exists(SPIDER_ROOT):
    #     process_dataset("spider", SPIDER_ROOT)
